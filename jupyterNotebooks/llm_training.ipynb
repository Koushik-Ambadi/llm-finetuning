{
 "cells": [
  {
   "cell_type": "raw",
   "id": "71b6d686-b637-48c9-90ab-0c46a3f58c74",
   "metadata": {},
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/flat_dataset/final_dataset.csv\")  # or your DataFrame\n",
    "dataset = Dataset.from_pandas(df[['requirement_description', 'test_steps']])\n",
    "\n",
    "\n",
    "def format_prompt(example):\n",
    "    return {\n",
    "        \"input\": f\"Write a test case for the following requirement:\\n{example['requirement_description']}\",\n",
    "        \"output\": example[\"test_steps\"]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_prompt)\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"/path/to/Mistral-3B-Instruct/\"  # update this\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"cpu\",\n",
    ")\n",
    "\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mistral3b-finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    bf16=False,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    max_seq_length=2048\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "trainer.model.save_pretrained(\"mistral3b-finetuned\")\n",
    "tokenizer.save_pretrained(\"mistral3b-finetuned\")\n",
    "\n",
    "\n",
    "prompt = \"Write a test case for the following requirement:\\nVerify login with valid credentials.\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(**inputs, max_new_tokens=300)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "327521d6-3f2f-47b4-a417-2ded78699ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koushik\\Desktop\\project\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16350/16350 [00:00<00:00, 22215.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"C://Users//koushik//Desktop//project//data//flat_dataset//final_dataset.csv\")\n",
    "dataset = Dataset.from_pandas(df[['requirement_description', 'test_steps']])\n",
    "\n",
    "# Format prompt\n",
    "def format_prompt(example):\n",
    "    return {\n",
    "        \"input\": f\"Write a test case for the following requirement:\\n{example['requirement_description']}\",\n",
    "        \"output\": example[\"test_steps\"]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_prompt)\n",
    "\n",
    "# Split the dataset (1% for testing)\n",
    "split_dataset = dataset.train_test_split(test_size=0.01, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f38c169a-6aab-48cd-913d-fc1d79b63cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "The 8-bit optimizer is not available on your device, only available on CUDA for now.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [02:32<00:00, 50.91s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_path = r\"C:\\Users\\koushik\\Desktop\\project\\Mistral-3B-Instruct-v0.2-init\"  # raw string for Windows path\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,load_in_4bit=True, local_files_only=True, device_map=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2f79526-a3c3-4dfb-a9d5-1eb5cf2236eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['requirement_description', 'test_steps', 'input', 'output']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7a28563-f94c-4ddd-855b-23aa74a749ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16186/16186 [03:00<00:00, 89.59 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:01<00:00, 94.13 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    input_text = example[\"input\"] if example[\"input\"] is not None else \"\"\n",
    "    output_text = example[\"output\"] if example[\"output\"] is not None else \"\"\n",
    "\n",
    "    combined_text = input_text + \"\\n\" + output_text\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        combined_text,\n",
    "        max_length=2048,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # force padding\n",
    "        return_attention_mask=True,  # ðŸ‘ˆ ensure attention_mask is returned\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    labels = tokenized[\"input_ids\"].copy()\n",
    "    labels = [label if label != tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],  # make sure it's here\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, remove_columns=[\"input\", \"output\", \"requirement_description\", \"test_steps\"])\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, remove_columns=[\"input\", \"output\", \"requirement_description\", \"test_steps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "050509ee-5b0c-4dc7-81ac-13d8c940aff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Add this right after tokenization:\n",
    "tokenized_train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ed3f1-d12f-4a7f-8125-bc3d4e072dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adfadff9-0a37-4b19-8e4b-a651adbf12f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "Train samples missing attention_mask: 0\n",
      "Eval samples missing attention_mask: 0\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset[0].keys())\n",
    "print(tokenized_test_dataset[0].keys())\n",
    "# Check how many samples are missing attention_mask\n",
    "missing_mask_train = [i for i, x in enumerate(tokenized_train_dataset) if \"attention_mask\" not in x]\n",
    "missing_mask_eval = [i for i, x in enumerate(tokenized_test_dataset) if \"attention_mask\" not in x]\n",
    "\n",
    "print(f\"Train samples missing attention_mask: {len(missing_mask_train)}\")\n",
    "print(f\"Eval samples missing attention_mask: {len(missing_mask_eval)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7220279a-9013-4b8b-a767-39d71f7ad5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 2,815,954,944 || trainable%: 0.1210\n"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "# Step 1: Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Step 2: Define LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # adjust for Mistral architecture\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Step 3: Attach LoRA to the quantized model\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# (optional) Print trainable parameters\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0782e372-92ac-4ec5-b682-e936f74d9360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokenized_train_dataset))\n",
    "print(type(tokenized_test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8de1c051-7f64-4b8e-a92d-9ba8328ae428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "Batch keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader = DataLoader(tokenized_train_dataset, batch_size=2, collate_fn=data_collator)\n",
    "\n",
    "for batch in test_loader:\n",
    "    print(\"Batch keys:\", batch.keys())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f35d8-63b3-4248-9b64-0b2dd7243ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncating train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16186/16186 [00:00<00:00, 116379.42 examples/s]\n",
      "Truncating eval dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:00<00:00, 20500.87 examples/s]\n",
      "Trainer.tokenizer is now deprecated. You should use `Trainer.processing_class = processing_class` instead.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "C:\\Users\\koushik\\Desktop\\project\\venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='12141' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    3/12141 28:38 < 5794:36:58, 0.00 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Your config with dataset_text_field=None because dataset is pre-tokenized\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./mistral3b-finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    bf16=False,\n",
    "    fp16=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    dataset_text_field=None\n",
    ")\n",
    "\n",
    "# Custom simple collator for tokenized dataset with labels\n",
    "def data_collator(features):\n",
    "    print(\"Sample keys:\", features[0].keys())  # Debug print\n",
    "    batch = {\n",
    "    \"input_ids\": torch.stack([f[\"input_ids\"] for f in features]),\n",
    "    \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in features]),\n",
    "    \"labels\": torch.stack([f[\"labels\"] for f in features]),\n",
    "    }\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    data_collator=None,\n",
    ")\n",
    "\n",
    "trainer.tokenizer = tokenizer  # optional, but recommended\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9130e6e-0e6e-4e11-9b6a-4ceeed9551da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral3b-finetuned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral3b-finetuned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.model.save_pretrained(\"mistral3b-finetuned\")\n",
    "tokenizer.save_pretrained(\"mistral3b-finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e8542-5e02-47f4-9a43-1f4f42c6beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a test case for the following requirement:\\nVerify login with valid credentials.\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=300)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
