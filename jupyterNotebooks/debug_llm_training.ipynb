{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebeb63f2-4bac-41ed-84cd-6e6e2f4d8682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/navpc24/Desktop/llm-finetuning/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from functools import partial\n",
    "\n",
    "# Make sure CUDA_VISIBLE_DEVICES is set to use only GPU 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Add project root to path so we can import your modules\n",
    "import sys\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # assuming notebook is inside jupyterNotebooks/\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Import pipeline helper functions and config\n",
    "from transformers import set_seed\n",
    "from model_training.config import (\n",
    "    csv_path, output_dir, train_batch_size,\n",
    "    gradient_accumulation, num_epochs, learning_rate, model_path,truncation_side,padding_side\n",
    ")\n",
    "from model_training.dataset_utils import load_dataset, preprocess_dataset,format_prompt\n",
    "from model_training.tokenizer_utils import load_tokenizer, tokenize_function\n",
    "from model_training.model_utils import load_model\n",
    "from model_training.logging_utils import setup_main_logger, start_hardware_logging\n",
    "\n",
    "print(\"Imports done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1862fa7-ce95-41aa-b246-6449576a7b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "print(\"Seed set to 42\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43549966-0674-4fdc-bacf-95cc96e57ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 16:15:03,357 - INFO - Starting training pipeline\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# For notebook, we'll keep it simple — log to console instead of file\n",
    "logger = logging.getLogger(\"train_pipeline\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.hasHandlers():\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "logger.info(\"Starting training pipeline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96f5e5c8-5661-46e6-9be9-f9892813e357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 16:15:10,277 - INFO - Started hardware logging thread\n"
     ]
    }
   ],
   "source": [
    "stop_event, log_thread = start_hardware_logging(output_dir, interval=5)\n",
    "logger.info(\"Started hardware logging thread\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4da4919-036e-46ca-a6b2-82c476b49457",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m(csv_path)\n\u001b[1;32m      2\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset sample:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdataset[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(csv_path)\n",
    "logger.info(\"Loaded dataset\")\n",
    "print(f\"Dataset sample:\\n{dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad42e29-82b6-4ee6-84dd-872059e8a066",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = preprocess_dataset(dataset)\n",
    "logger.info(\"Split dataset into train/test\")\n",
    "\n",
    "print(\"Split dataset keys:\", split_dataset.keys())\n",
    "print(f\"Train size: {len(split_dataset['train'])}\")\n",
    "print(f\"Test size: {len(split_dataset['test'])}\")\n",
    "print(\"\\nSample train record:\")\n",
    "print(split_dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e8bec-c45a-4b3e-acc2-49650078d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from model_training.tokenizer_utils import load_tokenizer, tokenize_function\n",
    "from functools import partial\n",
    "dummy_data = {\n",
    "    \"input\": [\n",
    "        \"Add two numbers.\",  # Small\n",
    "        \"Add two numbers. Ensure the function handles both positive and negative integers.\",  # Medium\n",
    "        \"Add two numbers. Ensure the function handles both positive and negative integers. The function should also support very large numbers, be optimized for performance, handle edge cases such as integer overflows, and provide meaningful error messages in case of invalid input. Additionally, it should be covered by unit tests and integration tests to ensure correctness.\"  # Large\n",
    "    ],\n",
    "    \"output\": [\n",
    "        \"Return their sum.\",  # Small\n",
    "        \"Return the sum of the numbers, accounting for positive and negative values.\",  # Medium\n",
    "        \"Return the sum of the numbers, ensuring correctness across all edge cases, including large integer values, and throw an appropriate exception in case of invalid inputs like None or non-integer types.\"  # Large\n",
    "    ],\n",
    "    \"requirement_description\": [\n",
    "        \"Write a function that takes two integers and returns their sum.\",\n",
    "        \"Write a function that takes two integers and returns their sum. Ensure that the implementation is safe for all valid integer inputs.\",\n",
    "        \"Write a function that takes two integers and returns their sum. The function must pass rigorous testing, including randomized tests, performance benchmarks, and edge case scenarios such as maximum and minimum integers.\"\n",
    "    ],\n",
    "    \"test_steps\": [\n",
    "        \"Input: 2, 3 -> Output: 5\",\n",
    "        \"Input: -1, 4 -> Output: 3; Input: 0, 0 -> Output: 0\",\n",
    "        \"Input: 2147483647, 1 -> Output: Error; Input: -2147483648, -1 -> Output: Error\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to HuggingFace dataset\n",
    "dummy_dataset = Dataset.from_dict(dummy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8132eec-d26b-4e62-a584-378c99d6fccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer()\n",
    "\n",
    "tokenized_dataset = dummy_dataset.map(\n",
    "    partial(tokenize_function, tokenizer=tokenizer),\n",
    "    remove_columns=[\"input\", \"output\", \"requirement_description\", \"test_steps\"],\n",
    "    batched=True,\n",
    "    batch_size=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d09dbfe-9133-49bd-982a-ea0dace827b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b79858fa-fd1c-400d-b997-66873be8be71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c098cc4e0116450a941f71db0982f2f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Trainable Parameters Check]\n",
      "[ ] base_model.model.model.embed_tokens.weight | shape: torch.Size([32000, 4096])\n",
      "[ ] base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.0.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.0.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.0.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.0.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.0.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.0.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.0.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.1.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.1.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.1.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.1.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.1.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.1.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.1.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.2.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.2.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.2.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.2.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.2.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.2.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.2.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.3.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.3.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.3.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.3.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.3.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.3.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.3.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.4.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.4.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.4.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.4.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.4.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.4.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.4.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.5.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.5.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.5.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.5.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.5.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.5.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.5.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.6.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.6.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.6.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.6.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.6.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.6.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.6.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.7.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.7.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.7.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.7.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.7.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.7.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.7.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.8.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.8.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.8.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.8.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.8.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.8.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.8.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.9.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.9.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.9.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.9.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.9.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.9.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.9.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.10.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.10.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.10.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.10.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.10.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.10.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.10.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.11.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.11.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.11.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.11.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.11.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.11.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.11.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.12.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.12.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.12.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.12.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.12.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.12.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.12.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.13.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.13.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.13.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.13.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.13.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.13.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.13.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.14.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.14.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.14.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.14.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.14.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.14.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.14.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.15.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.15.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.15.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.15.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.15.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.15.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.15.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.16.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.16.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.16.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.16.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.16.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.16.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.16.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.17.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.17.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.17.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.17.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.17.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.17.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.17.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.18.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.18.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.18.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.18.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.18.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.18.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.18.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.19.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.19.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.19.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.19.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.19.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.19.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.19.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.20.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.20.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.20.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.20.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.20.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.20.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.20.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.21.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.21.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.21.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.21.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.21.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.21.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.21.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.22.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.22.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.22.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.22.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.22.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.22.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.22.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.23.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.23.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.23.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.23.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.23.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.23.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.23.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.24.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.24.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.24.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.24.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.24.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.24.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.24.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.25.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.25.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.25.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.25.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.25.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.25.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.25.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.26.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.26.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.26.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.26.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.26.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.26.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.26.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.27.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.27.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.27.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.27.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.27.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.27.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.27.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.28.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.28.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.28.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.28.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.28.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.28.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.28.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.29.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.29.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.29.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.29.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.29.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.29.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.29.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.30.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.30.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.30.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.30.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.30.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.30.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.30.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight | shape: torch.Size([8388608, 1])\n",
      "[✓] base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight | shape: torch.Size([4096, 8])\n",
      "[ ] base_model.model.model.layers.31.self_attn.k_proj.weight | shape: torch.Size([2097152, 1])\n",
      "[ ] base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight | shape: torch.Size([2097152, 1])\n",
      "[✓] base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight | shape: torch.Size([8, 4096])\n",
      "[✓] base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight | shape: torch.Size([1024, 8])\n",
      "[ ] base_model.model.model.layers.31.self_attn.o_proj.weight | shape: torch.Size([8388608, 1])\n",
      "[ ] base_model.model.model.layers.31.mlp.gate_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.31.mlp.up_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.31.mlp.down_proj.weight | shape: torch.Size([6291456, 1])\n",
      "[ ] base_model.model.model.layers.31.input_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.layers.31.post_attention_layernorm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.model.norm.weight | shape: torch.Size([4096])\n",
      "[ ] base_model.model.lm_head.weight | shape: torch.Size([32000, 4096])\n",
      "🔢 Total: 1,540,886,528 | Trainable: 3,407,872 (0.22%)\n"
     ]
    }
   ],
   "source": [
    "from model_training.model_utils import load_model\n",
    "def debug_trainable_params(model):\n",
    "    total = 0\n",
    "    trainable = 0\n",
    "    print(\"\\n[Trainable Parameters Check]\")\n",
    "    for name, param in model.named_parameters():\n",
    "        total += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable += param.numel()\n",
    "            print(f\"[✓] {name} | shape: {param.shape}\")\n",
    "        else:\n",
    "            print(f\"[ ] {name} | shape: {param.shape}\")\n",
    "    print(f\"🔢 Total: {total:,} | Trainable: {trainable:,} ({100 * trainable / total:.2f}%)\")\n",
    "\n",
    "# After loading model\n",
    "model = load_model()\n",
    "debug_trainable_params(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4194825c-9bab-40c4-b329-bbec50dcc054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): Linear4bit(in_features=4096, out_features=3072, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=4096, out_features=3072, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (rotary_emb): MistralRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da2f88-9c9a-4430-b697-1527adc555b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
